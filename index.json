[{"categories":["AWS"],"contents":"Before jumping into the why and how, here\u0026rsquo;s what we achieved:\n✅ Replaced a single point of failure with AWS-native components\n💸 Reduced monthly costs by 70%\n🧑‍💻 No more SSH, servers, or NGINX restarts\n🔐 Fully managed SSL with ACM\n🛠️ Everything codified and versioned via Terraform\nAll of this — without a single EC2 server or NGINX config file. Why we left EC2 + NGINX behind (and how much it saved us) We had a simple goal: redirect traffic for over 70 different domains.\nAt first, we solved it with what we knew — an EC2 instance running NGINX with hardcoded 301 and 302 rules. It worked… until it didn’t.\nWhat started as a quick fix became an operational headache:\nAny new redirect meant SSH access and manual file editing Restarting NGINX came with downtime risks A single typo could break everything Most importantly: we were maintaining a full server just to handle redirects Eventually, we asked ourselves: isn’t this just a glorified spreadsheet with SSL and uptime issues?\nThat question led us to redesign the whole setup using ALB, Terraform, ACM, and nothing else.\nAnd it worked — spectacularly. 💰 EC2 vs ALB: The Cost of Simplicity ❌ EC2 + NGINX ✅ ALB + Terraform Cost ~$73/mo ~$22.50/mo Access SSH/SSM required No SSH/SSM ever SSL Manual setup Auto SSL (ACM) Maintenance Manual file editing Fully versioned in Terraform Scalability Single point of failure Fully scalable rules Downtime NGINX restarts cause downtime Zero downtime on rule changes That’s a ~70% reduction, with zero servers to maintain and full scalability.\n🛠 The New Setup: ALB + Terraform + SSL + Logs Once we made the switch, we wanted the entire solution to be automated, secure, and easy to maintain. Here’s what we built:\n📊 Visual Overview: How the Flow Works Sometimes it\u0026rsquo;s easier to understand this type of infrastructure visually. Here\u0026rsquo;s a high-level diagram that shows how user requests travel through Route 53, the ALB, and how redirect behavior is handled:\nThis shows:\nWhat happens when someone accesses a random or known domain How Route 53 routes requests to the ALB The difference between traffic arriving on port 80 and port 443 What the ALB does when no matching redirect rule is found (default 404) 🔧 Components Used ALB (Application Load Balancer): Handles all HTTP/HTTPS requests Redirect Listener Rules: Define what gets redirected and where Route 53: Hosted Zones already managed all our domain DNS AWS ACM: For issuing and auto-renewing SSL certificates Access Logs: Enabled and pushed to S3 for auditing Terraform: Used to define everything as code Modules: We used Anton Babenko’s modules for alb and acm 🔐 Managing SSL Certificates with ACM We used ACM (AWS Certificate Manager) to request certificates per domain, with auto-renewal enabled. Since we had all domains in Route 53, validation was done automatically via DNS — no manual setup needed.\nCode:\nmodule \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; domain_name = \u0026#34;example.com\u0026#34; subject_alternative_names = [\u0026#34;www.example.com\u0026#34;] zone_id = data.aws_route53_zone.main.zone_id validate_certificate = true } 🌐 ALB Listener Rules + Redirects We implemented two listeners:\nPort 80 (HTTP): Redirects everything to HTTPS\nPort 443 (HTTPS): Processes redirect rules (up to 100 max), default action is 404 Code:\nmodule \u0026#34;redirect_alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;9.12.0\u0026#34; name = \u0026#34;redirect-alb\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; internal = false vpc_id = aws_vpc.main.id subnets = [aws_subnet.public_a.id, aws_subnet.public_b.id] access_logs = { bucket = aws_s3_bucket.alb_logs.bucket enabled = true prefix = \u0026#34;alb-access\u0026#34; } create_security_group = true security_group_ingress_rules = { http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } http_listeners = [ { port = 80 protocol = \u0026#34;HTTP\u0026#34; action = { type = \u0026#34;redirect\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } } ] https_listeners = [ { port = 443 protocol = \u0026#34;HTTPS\u0026#34; certificate_arn = module.acm.certificate_arn rules = [ { priority = 10 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.example.com\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;old-domain.com\u0026#34;] } }] }, { priority = 20 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.blog.example.com\u0026#34; path = \u0026#34;/blog\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;legacy-blog.net\u0026#34;] } }] } ] default_action = { type = \u0026#34;fixed-response\u0026#34; fixed_response = { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Not Found\u0026#34; status_code = \u0026#34;404\u0026#34; } } } ] } ✨ Why This Worked for Us ✅ Fully serverless — no EC2 to maintain\n✅ ~70% cost savings\n✅ All redirects are version-controlled in Terraform\n✅ SSL certificates managed automatically (with renewals) via ACM\n✅ Access logs enabled for auditing\n✅ Default rule returns 404 to avoid exposing internal details\n✅ Port 80 cleanly redirects to HTTPS (443)\n✅ Ready to integrate with WAF or CloudWatch in the future\n✅ Everything is Infrastructure-as-Code, ready for CI/CD pipelines\n❌ What We Didn\u0026rsquo;t Use (And Why) Lambda@Edge – great for complex logic at scale, but overkill for simple 301s. CloudFront Functions – less Terraform support, more limits. S3 Static Hosting – no built-in SSL unless fronted by CloudFront. In the end, simplicity won. ALB covered 100% of our use case — no Lambda, no CloudFront needed.\n📌 Scaling Tip AWS ALB supports up to 100 listener rules per listener.\nIf you need more, consider:\n• 🧩 Use multiple listeners (e.g., different ports or multiple ALBs)\n• 🚀 Move to CloudFront Functions or Lambda@Edge for large-scale redirects\n• 🗂️ Group redirects using wildcard domains or path patterns\nPlan your redirect strategy early to avoid scaling bottlenecks later.\n💡 Lessons Learned Keep infrastructure as simple as the problem requires — NGINX was overkill for redirects. ALB rules are powerful, but don’t scale infinitely. Plan limits early. Terraform made experimentation and rollback safe and auditable. Relying on managed services like ACM for SSL removed 90% of the ops burden. In short, we turned a fragile EC2 setup into a robust serverless system — saving money, effort, and future headaches.\n🙋‍♂️ Need Help With This? Considering a similar migration or want to simplify your redirect setup?\nWe\u0026rsquo;ve migrated this same pattern across multiple AWS accounts — and the ops team never looked back.\n💬 Let’s chat on LinkedIn — happy to help or exchange ideas. ","date":"July 14, 2025","hero":"/images/posts/hero.webp","permalink":"https://nserbin.github.io/posts/aws/aws-networking/from-nginx-to-alb/","summary":"\u003cp\u003eBefore jumping into the why and how, here\u0026rsquo;s what we achieved:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e✅ Replaced a single point of failure with AWS-native components\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e💸 Reduced monthly costs by 70%\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🧑‍💻 No more SSH, servers, or NGINX restarts\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🔐 Fully managed SSL with ACM\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🛠️ Everything codified and versioned via Terraform\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eAll of this — without a single EC2 server or NGINX config file.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"why-we-left-ec2--nginx-behind-and-how-much-it-saved-us\"\u003eWhy we left EC2 + NGINX behind (and how much it saved us)\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nWe had a simple goal: redirect traffic for over \u003cstrong\u003e70 different domains\u003c/strong\u003e.\u003cbr\u003e\nAt first, we solved it with what we knew — an \u003cstrong\u003eEC2 instance running NGINX\u003c/strong\u003e with hardcoded \u003ccode\u003e301\u003c/code\u003e and \u003ccode\u003e302\u003c/code\u003e rules. It worked… until it didn’t.\u003c/p\u003e","tags":["Load Balancer","Redirects","NGINX","EC2"],"title":"NGINX Out, ALB In - 70% Cheaper Redirects with Zero Servers"}]