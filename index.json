[{"categories":["AWS","Databases","Backup"],"contents":"Before diving into the why and how, here’s what we deliver:\n⏱️ Clear diagnosis in under a minute (no console guesswork)\n🫥 Non-intrusive path: temporary instance only (we don’t touch your current RDS)\n🔐 Export that works, with the correct IAM/KMS shape\n✅ Copy-paste commands for your runbook or CI\nAll of this without modifying your existing DB instance — restore the snapshot into a temporary gp3 instance 🚀, create a new snapshot 📸, and export that one 📦. 🔒 Why the export is disabled The most frequent blocker is simple: the snapshot was taken from an instance on magnetic storage. Snapshots with that lineage aren’t eligible for export. The console is vague; the CLI tells you exactly what’s going on.\n⚖️ Console vs CLI: what actually helps ❌ Console only ✅ CLI runbook Feedback Greyed-out button, no reason why Explicit status \u0026 fields (StorageType, errors) Actions Trial and error Deterministic flow (restore → snapshot → export) Repeatability Manual steps Scriptable \u0026 auditable 🕵️‍♂️ Fast diagnosis (CLI) Check the snapshot storage type:\naws rds describe-db-snapshots \\ --db-snapshot-identifier \u0026lt;SNAPSHOT_ID\u0026gt; \\ --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,Engine,EngineVersion,StorageType,Encrypted,Status]\u0026#39; \\ --output table If StorageType = standard, that snapshot won’t export. 🧭 Architecture at a glance The flow is straightforward: restore snapshot → temporary gp3 instance → new snapshot → export to S3 with an export role trusted by export.rds.amazonaws.com.\nThis shows:\n🔁 Where the storage lineage changes (magnetic → gp3)\n🔗 How the export service assumes an IAM role to write to S3\n🧰 The minimal moving parts (RDS, S3, IAM, optional KMS)\n📘 The playbook (no changes to your existing RDS) We do not modify the current DB instance. We restore the snapshot into a temporary SSD-backed instance, create a new snapshot, and export that one. 🧱 Restore the snapshot on gp3 (temporary instance) # Vars export REGION=\u0026lt;REGION\u0026gt; export TEMP_DB_ID=\u0026lt;TEMP_DB_ID\u0026gt; export SNAPSHOT_ID=\u0026lt;SNAPSHOT_ID\u0026gt; export SUBNET_GROUP=\u0026lt;DB_SUBNET_GROUP\u0026gt; export SG_ID=\u0026lt;SECURITY_GROUP_ID\u0026gt; aws rds restore-db-instance-from-db-snapshot \\ --region $REGION \\ --db-instance-identifier $TEMP_DB_ID \\ --db-snapshot-identifier $SNAPSHOT_ID \\ --storage-type gp3 \\ --no-publicly-accessible \\ --db-subnet-group-name $SUBNET_GROUP \\ --vpc-security-group-ids $SG_ID 📸 Create a new snapshot (now SSD-backed) export NEW_SNAPSHOT_ID=${TEMP_DB_ID}-exportable-$(date +%Y%m%d) aws rds create-db-snapshot \\ --db-snapshot-identifier $NEW_SNAPSHOT_ID \\ --db-instance-identifier $TEMP_DB_ID # Sanity check aws rds describe-db-snapshots \\ --db-snapshot-identifier $NEW_SNAPSHOT_ID \\ --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,StorageType,Status]\u0026#39; \\ --output table 🧷 Export to S3 (the wiring that matters) ✅ Requirements 🌍 S3 bucket in the same region as the snapshot\n🤝 Export IAM role trusted by export.rds.amazonaws.com\n📝 Role permissions to write to S3 (PutObject, ListBucket, GetBucketLocation, etc.)\n🔑 If using KMS, key Enabled and allows the export service\n🛑 No Organization SCP blocking rds:StartExportTask\n👇 Trust policy (assumed by the export service) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;export.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } 📜 Minimal permissions policy (S3 + KMS) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject*\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:DeleteObject*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;KMS_KEY_ARN\u0026gt;\u0026#34; } ] } 🧾 Bucket policy (explicit allow for the role) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowRDSExportWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt;\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] } ] } ▶️ Start the export aws rds start-export-task \\ --export-task-identifier export-$NEW_SNAPSHOT_ID \\ --source-arn arn:aws:rds:$REGION:\u0026lt;ACCOUNT_ID\u0026gt;:snapshot:$NEW_SNAPSHOT_ID \\ --s3-bucket-name \u0026lt;BUCKET\u0026gt; \\ --s3-prefix rds-exports/$NEW_SNAPSHOT_ID/ \\ --iam-role-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt; \\ --kms-key-id \u0026lt;KMS_KEY_ARN\u0026gt; 👀 Observe the run aws rds describe-export-tasks \\ --filters Name=export-task-identifier,Values=export-$NEW_SNAPSHOT_ID \\ --query \u0026#39;ExportTasks[0].[Status,ProgressPercentage,S3Bucket,S3Prefix,FailureCause]\u0026#39; \\ --output table 🧯 Troubleshooting (quick triage) 🔎 AccessDenied → check Organization SCPs, caller permissions, and the export role policy 🧱 KMSKeyNotAccessible → key disabled or a deny affecting export.rds.amazonaws.com 🗺️ BucketRegionError → snapshot and bucket must be in the same region 🤝 Shared snapshot → align KMS access/grants cross-account 💡 Lessons Learned Don’t rely on the console to explain the “why” — the CLI is your source of truth Prevent this upfront: standardize on gp3 in modules and block standard storage in CI Keep a pre-approved export role per account/region to avoid one-off IAM changes 🙋‍♂️ Need Help With This? Need help maintaining your RDS setup - keeping backups restorable, snapshot governance tight, etc. ?\nWe’ve implemented these patterns across multiple AWS accounts and regions — repeatable, auditable, and low‑touch.\n💬 Let’s chat on LinkedIn — happy to help or exchange ideas. ","date":"August 27, 2025","hero":"/images/posts/rds-to-s3.webp","permalink":"https://nserbin.github.io/posts/aws/aws-rds/export-to-s3/","summary":"\u003cp\u003eBefore diving into the why and how, here’s what we deliver:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e⏱️ Clear diagnosis in under a minute (no console guesswork)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🫥 Non-intrusive path: \u003cstrong\u003etemporary\u003c/strong\u003e instance only (we don’t touch your current RDS)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🔐 Export that works, with the correct \u003cstrong\u003eIAM/KMS\u003c/strong\u003e shape\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e✅ Copy-paste commands for your runbook or CI\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"margin-top: 2rem;\"\u003e\u003c/div\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eAll of this without modifying your existing DB instance — restore the snapshot into a temporary \u003cstrong\u003egp3\u003c/strong\u003e instance 🚀, create a new snapshot 📸, and export that one 📦.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 4rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"-why-the-export-is-disabled\"\u003e🔒 Why the export is disabled\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nThe most frequent blocker is simple: the snapshot was taken from an instance on \u003cstrong\u003emagnetic storage\u003c/strong\u003e. Snapshots with that lineage aren’t eligible for export. The console is vague; the CLI tells you exactly what’s going on.\u003c/p\u003e","tags":["S3","RDS","Storage Type","Export to S3","Snapshot"],"title":"RDS Snapshot → S3: Diagnose and Fix a Disabled Export"},{"categories":["AWS"],"contents":"Before jumping into the why and how, here\u0026rsquo;s what we achieved:\n✅ Replaced a single point of failure with AWS-native components\n💸 Reduced monthly costs by 70%\n🧑‍💻 No more SSH, servers, or NGINX restarts\n🔐 Fully managed SSL with ACM\n🛠️ Everything codified and versioned via Terraform\nAll of this — without a single EC2 server or NGINX config file. Why we left EC2 + NGINX behind (and how much it saved us) We had a simple goal: redirect traffic for over 70 different domains.\nAt first, we solved it with what we knew — an EC2 instance running NGINX with hardcoded 301 and 302 rules. It worked… until it didn’t.\nWhat started as a quick fix became an operational headache:\nAny new redirect meant SSH access and manual file editing Restarting NGINX came with downtime risks A single typo could break everything Most importantly: we were maintaining a full server just to handle redirects Eventually, we asked ourselves: isn’t this just a glorified spreadsheet with SSL and uptime issues?\nThat question led us to redesign the whole setup using ALB, Terraform, ACM, and nothing else.\nAnd it worked — spectacularly. 💰 EC2 vs ALB: The Cost of Simplicity ❌ EC2 + NGINX ✅ ALB + Terraform Cost ~$73/mo ~$22.50/mo Access SSH/SSM required No SSH/SSM ever SSL Manual setup Auto SSL (ACM) Maintenance Manual file editing Fully versioned in Terraform Scalability Single point of failure Fully scalable rules Downtime NGINX restarts cause downtime Zero downtime on rule changes That’s a ~70% reduction, with zero servers to maintain and full scalability.\n🛠 The New Setup: ALB + Terraform + SSL + Logs Once we made the switch, we wanted the entire solution to be automated, secure, and easy to maintain. Here’s what we built:\n📊 Visual Overview: How the Flow Works Sometimes it\u0026rsquo;s easier to understand this type of infrastructure visually. Here\u0026rsquo;s a high-level diagram that shows how user requests travel through Route 53, the ALB, and how redirect behavior is handled:\nThis shows:\nWhat happens when someone accesses a random or known domain How Route 53 routes requests to the ALB The difference between traffic arriving on port 80 and port 443 What the ALB does when no matching redirect rule is found (default 404) 🔧 Components Used ALB (Application Load Balancer): Handles all HTTP/HTTPS requests Redirect Listener Rules: Define what gets redirected and where Route 53: Hosted Zones already managed all our domain DNS AWS ACM: For issuing and auto-renewing SSL certificates Access Logs: Enabled and pushed to S3 for auditing Terraform: Used to define everything as code Modules: We used Anton Babenko’s modules for alb and acm 🔐 Managing SSL Certificates with ACM We used ACM (AWS Certificate Manager) to request certificates per domain, with auto-renewal enabled. Since we had all domains in Route 53, validation was done automatically via DNS — no manual setup needed.\nCode:\nmodule \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; domain_name = \u0026#34;example.com\u0026#34; subject_alternative_names = [\u0026#34;www.example.com\u0026#34;] zone_id = data.aws_route53_zone.main.zone_id validate_certificate = true } 🌐 ALB Listener Rules + Redirects We implemented two listeners:\nPort 80 (HTTP): Redirects everything to HTTPS\nPort 443 (HTTPS): Processes redirect rules (up to 100 max), default action is 404 Code:\nmodule \u0026#34;redirect_alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;9.12.0\u0026#34; name = \u0026#34;redirect-alb\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; internal = false vpc_id = aws_vpc.main.id subnets = [aws_subnet.public_a.id, aws_subnet.public_b.id] access_logs = { bucket = aws_s3_bucket.alb_logs.bucket enabled = true prefix = \u0026#34;alb-access\u0026#34; } create_security_group = true security_group_ingress_rules = { http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } http_listeners = [ { port = 80 protocol = \u0026#34;HTTP\u0026#34; action = { type = \u0026#34;redirect\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } } ] https_listeners = [ { port = 443 protocol = \u0026#34;HTTPS\u0026#34; certificate_arn = module.acm.certificate_arn rules = [ { priority = 10 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.example.com\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;old-domain.com\u0026#34;] } }] }, { priority = 20 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.blog.example.com\u0026#34; path = \u0026#34;/blog\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;legacy-blog.net\u0026#34;] } }] } ] default_action = { type = \u0026#34;fixed-response\u0026#34; fixed_response = { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Not Found\u0026#34; status_code = \u0026#34;404\u0026#34; } } } ] } ✨ Why This Worked for Us ✅ Fully serverless — no EC2 to maintain\n✅ ~70% cost savings\n✅ All redirects are version-controlled in Terraform\n✅ SSL certificates managed automatically (with renewals) via ACM\n✅ Access logs enabled for auditing\n✅ Default rule returns 404 to avoid exposing internal details\n✅ Port 80 cleanly redirects to HTTPS (443)\n✅ Ready to integrate with WAF or CloudWatch in the future\n✅ Everything is Infrastructure-as-Code, ready for CI/CD pipelines\n❌ What We Didn\u0026rsquo;t Use (And Why) Lambda@Edge – great for complex logic at scale, but overkill for simple 301s. CloudFront Functions – less Terraform support, more limits. S3 Static Hosting – no built-in SSL unless fronted by CloudFront. In the end, simplicity won. ALB covered 100% of our use case — no Lambda, no CloudFront needed.\n📌 Scaling Tip AWS ALB supports up to 100 listener rules per listener.\nIf you need more, consider:\n• 🧩 Use multiple listeners (e.g., different ports or multiple ALBs)\n• 🚀 Move to CloudFront Functions or Lambda@Edge for large-scale redirects\n• 🗂️ Group redirects using wildcard domains or path patterns\nPlan your redirect strategy early to avoid scaling bottlenecks later.\n💡 Lessons Learned Keep infrastructure as simple as the problem requires — NGINX was overkill for redirects. ALB rules are powerful, but don’t scale infinitely. Plan limits early. Terraform made experimentation and rollback safe and auditable. Relying on managed services like ACM for SSL removed 90% of the ops burden. In short, we turned a fragile EC2 setup into a robust serverless system — saving money, effort, and future headaches.\n🙋‍♂️ Need Help With This? Considering a similar migration or want to simplify your redirect setup?\nWe\u0026rsquo;ve migrated this same pattern across multiple AWS accounts — and the ops team never looked back.\n💬 Let’s chat on LinkedIn — happy to help or exchange ideas. ","date":"July 14, 2025","hero":"/images/posts/nginx-to-alb.webp","permalink":"https://nserbin.github.io/posts/aws/aws-networking/from-nginx-to-alb/","summary":"\u003cp\u003eBefore jumping into the why and how, here\u0026rsquo;s what we achieved:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e✅ Replaced a single point of failure with AWS-native components\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e💸 Reduced monthly costs by 70%\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🧑‍💻 No more SSH, servers, or NGINX restarts\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🔐 Fully managed SSL with ACM\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🛠️ Everything codified and versioned via Terraform\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eAll of this — without a single EC2 server or NGINX config file.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"why-we-left-ec2--nginx-behind-and-how-much-it-saved-us\"\u003eWhy we left EC2 + NGINX behind (and how much it saved us)\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nWe had a simple goal: redirect traffic for over \u003cstrong\u003e70 different domains\u003c/strong\u003e.\u003cbr\u003e\nAt first, we solved it with what we knew — an \u003cstrong\u003eEC2 instance running NGINX\u003c/strong\u003e with hardcoded \u003ccode\u003e301\u003c/code\u003e and \u003ccode\u003e302\u003c/code\u003e rules. It worked… until it didn’t.\u003c/p\u003e","tags":["Load Balancer","Redirects","NGINX","EC2"],"title":"NGINX Out, ALB In - 70% Cheaper Redirects with Zero Servers"}]