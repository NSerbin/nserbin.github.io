[{"categories":["AWS","Databases","Backup"],"contents":"Antes de entrar en el por qué y el cómo, esto es lo que entregamos:\n⏱️ Diagnóstico claro en menos de un minuto (sin adivinar en la consola)\n🫥 Camino no intrusivo: solo instancia temporal (no tocamos tu RDS actual)\n🔐 Export que funciona, con la forma correcta de IAM/KMS\n🧩 Comandos copy-paste para tu runbook o CI\nTodo esto sin modificar tu instancia actual: restaurás el Snapshot en una instancia gp3 temporal 🚀, creás un Snapshot nuevo 📸 y exportás ese 📦. Por qué el Export está deshabilitado 🔒 El bloqueo más frecuente es simple: el Snapshot fue tomado desde una instancia con almacenamiento magnético (standard) 🧲. Los Snapshots con ese linaje no son elegibles para Export. La consola es ambigua; el CLI te muestra exactamente qué pasa 🧪.\nConsole vs CLI: lo que realmente ayuda ⚖️ ❌ Solo consola ✅ Runbook con CLI Feedback Botón gris, sin motivo claro 😶‍🌫️ Estado y campos explícitos (StorageType, errores) 🧾 Acciones Prueba y error 🎯 Flujo determinístico (restore → Snapshot → export) ▶️ Repeatability Pasos manuales 🧱 Scriptable \u0026 auditable 🤖 Diagnóstico rápido (CLI) 🕵️‍♂️ Revisar el tipo de almacenamiento del Snapshot\naws rds describe-db-snapshots --db-snapshot-identifier \u0026lt;SNAPSHOT_ID\u0026gt; --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,Engine,EngineVersion,StorageType,Encrypted,Status]\u0026#39; --output table Si StorageType = standard, ese Snapshot no se va a exportar. Arquitectura a simple vista 🧭 El flujo es directo: restore del Snapshot → instancia gp3 temporal → nuevo Snapshot → Export a S3 con un IAM Role asumido por export.rds.amazonaws.com.\nEsto muestra:\n🔁 Dónde cambia el linaje de almacenamiento (magnético → gp3)\n🔗 Cómo el servicio de export asume un IAM Role para escribir en un S3 Bucket\n🧰 Las piezas mínimas (RDS, S3, IAM, KMS opcional)\nPlaybook (sin cambios sobre tu RDS existente) 📘 No modificamos tu DB actual. Restauramos el Snapshot en una instancia SSD temporal, creamos un Snapshot nuevo y exportamos ese. 🧱 Restore del Snapshot en gp3 (instancia temporal) # Vars export REGION=\u0026lt;REGION\u0026gt; export TEMP_DB_ID=\u0026lt;TEMP_DB_ID\u0026gt; export SNAPSHOT_ID=\u0026lt;SNAPSHOT_ID\u0026gt; export SUBNET_GROUP=\u0026lt;DB_SUBNET_GROUP\u0026gt; export SG_ID=\u0026lt;SECURITY_GROUP_ID\u0026gt; aws rds restore-db-instance-from-db-snapshot --region $REGION --db-instance-identifier $TEMP_DB_ID --db-snapshot-identifier $SNAPSHOT_ID --storage-type gp3 --no-publicly-accessible --db-subnet-group-name $SUBNET_GROUP --vpc-security-group-ids $SG_ID 📸 Crear un nuevo Snapshot (ahora SSD) export NEW_SNAPSHOT_ID=${TEMP_DB_ID}-exportable-$(date +%Y%m%d) aws rds create-db-snapshot --db-snapshot-identifier $NEW_SNAPSHOT_ID --db-instance-identifier $TEMP_DB_ID # Sanity check aws rds describe-db-snapshots --db-snapshot-identifier $NEW_SNAPSHOT_ID --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,StorageType,Status]\u0026#39; --output table 🧷 Export a S3 (el wiring que importa) ✅ Requerimientos 🌍 S3 Bucket en la misma región que el Snapshot\n🤝 IAM Role para Export con trust en export.rds.amazonaws.com\n📝 El Role puede escribir en el S3 Bucket (PutObject, ListBucket, GetBucketLocation, etc.)\n🔑 Si usás KMS, la KMS Key está Enabled y permite al servicio de export\n🛑 Sin SCP de la Organization bloqueando rds:StartExportTask\n👇 Trust Policy (asumida por el servicio de export) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;export.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } 📜 Minimal Permissions Policy (S3 + KMS) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject*\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:DeleteObject*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;KMS_KEY_ARN\u0026gt;\u0026#34; } ] } 🧾 Bucket Policy (allow explícito para el Role) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowRDSExportWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt;\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] } ] } ▶️ Iniciar el Export aws rds start-export-task \\ --export-task-identifier export-$NEW_SNAPSHOT_ID \\ --source-arn arn:aws:rds:$REGION:\u0026lt;ACCOUNT_ID\u0026gt;:snapshot:$NEW_SNAPSHOT_ID \\ --s3-bucket-name \u0026lt;BUCKET\u0026gt; \\ --s3-prefix rds-exports/$NEW_SNAPSHOT_ID/ \\ --iam-role-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt; \\ --kms-key-id \u0026lt;KMS_KEY_ARN\u0026gt; 👀 Observar la ejecución aws rds describe-export-tasks \\ --filters Name=export-task-identifier,Values=export-$NEW_SNAPSHOT_ID \\ --query \u0026#39;ExportTasks[0].[Status,ProgressPercentage,S3Bucket,S3Prefix,FailureCause]\u0026#39; \\ --output table Troubleshooting (triage rápido) 🧯 🔎 AccessDenied → revisá SCP de la Organization, permisos del caller y la Policy del IAM Role\n🧱 KMSKeyNotAccessible → KMS Key deshabilitada o algún Deny afectando export.rds.amazonaws.com\n🗺️ BucketRegionError → Snapshot y S3 Bucket deben estar en la misma región\n🤝 Shared snapshot → alinear accesos/grants de KMS para cross-account\n💡 Lecciones aprendidas No dependas de la consola para explicar el “por qué” — el CLI es tu fuente de verdad Prevenilo de entrada: estandarizá gp3 en tus módulos y bloqueá standard en CI Mantené un export Role preaprobado por cuenta/región para evitar IAM ad-hoc 🙋‍♂️ ¿Necesitás ayuda con esto? ¿Necesitás ayuda manteniendo tu RDS — Backups restaurables, governance de Snapshots, exports predecibles, etc. ?\nImplementamos estos patrones en múltiples cuentas y regiones de AWS — repetibles, auditables y de bajo mantenimiento.\n💬 Hablemos en LinkedIn — encantado de ayudar o intercambiar ideas. ","date":"August 27, 2025","hero":"/images/default-hero.jpg","permalink":"https://nserbin.github.io/es/posts/aws/aws-rds/export-to-s3/","summary":"\u003cp\u003eAntes de entrar en el por qué y el cómo, esto es lo que entregamos:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e⏱️ Diagnóstico claro en menos de un minuto (sin adivinar en la consola)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🫥 Camino no intrusivo: solo instancia \u003cstrong\u003etemporal\u003c/strong\u003e (no tocamos tu RDS actual)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🔐 Export que funciona, con la forma correcta de \u003cstrong\u003eIAM/KMS\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🧩 Comandos copy-paste para tu runbook o CI\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"margin-top: 2rem;\"\u003e\u003c/div\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eTodo esto sin modificar tu instancia actual: restaurás el Snapshot en una instancia \u003cstrong\u003egp3\u003c/strong\u003e temporal 🚀, creás un Snapshot nuevo 📸 y exportás ese 📦.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 4rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"por-qué-el-export-está-deshabilitado-\"\u003ePor qué el Export está deshabilitado 🔒\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nEl bloqueo más frecuente es simple: el Snapshot fue tomado desde una instancia con almacenamiento \u003cstrong\u003emagnético (\u003ccode\u003estandard\u003c/code\u003e)\u003c/strong\u003e 🧲. Los Snapshots con ese linaje \u003cstrong\u003eno\u003c/strong\u003e son elegibles para Export. La consola es ambigua; el \u003cstrong\u003eCLI\u003c/strong\u003e te muestra exactamente qué pasa 🧪.\u003c/p\u003e","tags":["S3","RDS","Storage Type","Export to S3","Snapshot"],"title":"RDS Snapshot → S3: Diagnosticar y resolver un Export deshabilitado"},{"categories":["AWS"],"contents":"Antes de entrar en el por qué y el cómo, esto es lo que logramos:\n✅ Reemplazamos un punto único de falla con componentes nativos de AWS\n💸 Reducimos los costos mensuales en un 70%\n🧑‍💻 No más SSH, servidores ni reinicios de NGINX\n🔐 SSL totalmente gestionado con ACM\n🛠️ Todo codificado y versionado con Terraform\nTodo esto — sin un solo servidor EC2 ni archivo de configuración NGINX. Por qué dejamos EC2 + NGINX (y cuánto nos ahorramos) Teníamos un objetivo simple: redirigir tráfico de más de 70 dominios diferentes.Primero lo resolvimos con lo que conocíamos — una instancia EC2 corriendo NGINX con reglas 301 y 302 hardcodeadas. Funcionó… hasta que dejó de funcionar.\nLo que empezó como una solución rápida se convirtió en un dolor de cabeza operativo:\nCualquier nueva redirección requería acceso por SSH y edición manual de archivos Reiniciar NGINX implicaba riesgos de downtime Un solo typo podía romper todo Y lo más importante: estábamos manteniendo un servidor completo solo para manejar redirecciones Finalmente nos preguntamos: ¿esto no es solo una planilla glorificada con SSL y problemas de uptime?Esa pregunta nos llevó a rediseñar todo con ALB, Terraform, ACM y nada más.\nY funcionó — espectacularmente. 💰 EC2 vs ALB: El costo de la simplicidad ❌ EC2 + NGINX ✅ ALB + Terraform Costo ~$73/mes ~$22.50/mes Acceso Requiere SSH/SSM No requiere SSH/SSM SSL Configuración manual SSL automático (ACM) Mantenimiento Edición manual de archivos Totalmente versionado en Terraform Escalabilidad Punto único de falla Reglas totalmente escalables Downtime Reinicios de NGINX causan downtime Cero downtime en cambios de reglas Eso representa una reducción de ~70%, sin servidores que mantener y con escalabilidad total.\n🛠 La nueva solución: ALB + Terraform + SSL + Logs Una vez que migramos, queríamos que la solución fuera automatizada, segura y fácil de mantener. Esto fue lo que construimos:\n📊 Vista general: cómo fluye el tráfico A veces es más fácil entender este tipo de infraestructura visualmente. Acá tenés un diagrama de alto nivel que muestra cómo viajan los requests del usuario a través de Route 53, el ALB, y cómo se manejan las redirecciones:\nEste diagrama muestra:\nQué pasa cuando alguien accede a un dominio conocido o no registrado Cómo Route 53 enruta los requests al ALB La diferencia entre el tráfico que llega por el puerto 80 y el puerto 443 Qué hace el ALB cuando no encuentra una regla de redirección (retorna 404) 🔧 Componentes utilizados ALB (Application Load Balancer): Maneja todos los requests HTTP/HTTPS Redirect Listener Rules: Definen qué se redirige y hacia dónde Route 53: Hosted Zones ya manejaban el DNS de todos nuestros dominios AWS ACM: Para emitir y renovar automáticamente los certificados SSL Access Logs: Activados y enviados a S3 para auditoría Terraform: Usado para definir toda la infraestructura como código Modules: Usamos los módulos de Anton Babenko para alb y acm 🔐 Gestión de certificados SSL con ACM Utilizamos ACM (AWS Certificate Manager) para solicitar certificados por dominio, con renovación automática activada. Como ya teníamos los dominios en Route 53, la validación se hizo automáticamente por DNS — sin configuración manual.\nCódigo:\nmodule \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; domain_name = \u0026#34;example.com\u0026#34; subject_alternative_names = [\u0026#34;www.example.com\u0026#34;] zone_id = data.aws_route53_zone.main.zone_id validate_certificate = true } 🌐 Listener Rules + Redirects con ALB Implementamos dos listeners:\nPuerto 80 (HTTP): Redirige todo a HTTPS\nPuerto 443 (HTTPS): Procesa reglas de redirect (máximo 100), con acción por defecto 404\nCódigo:\nmodule \u0026#34;redirect_alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;9.12.0\u0026#34; name = \u0026#34;redirect-alb\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; internal = false vpc_id = aws_vpc.main.id subnets = [aws_subnet.public_a.id, aws_subnet.public_b.id] access_logs = { bucket = aws_s3_bucket.alb_logs.bucket enabled = true prefix = \u0026#34;alb-access\u0026#34; } create_security_group = true security_group_ingress_rules = { http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } http_listeners = [ { port = 80 protocol = \u0026#34;HTTP\u0026#34; action = { type = \u0026#34;redirect\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } } ] https_listeners = [ { port = 443 protocol = \u0026#34;HTTPS\u0026#34; certificate_arn = module.acm.certificate_arn rules = [ { priority = 10 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.example.com\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;old-domain.com\u0026#34;] } }] }, { priority = 20 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.blog.example.com\u0026#34; path = \u0026#34;/blog\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;legacy-blog.net\u0026#34;] } }] } ] default_action = { type = \u0026#34;fixed-response\u0026#34; fixed_response = { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Not Found\u0026#34; status_code = \u0026#34;404\u0026#34; } } } ] } ✨ Por qué funcionó para nosotros ✅ Completamente serverless — sin EC2 que mantener\n✅ ~70% de ahorro en costos\n✅ Todos los redirects están versionados con Terraform\n✅ Certificados SSL gestionados automáticamente (con renovaciones) vía ACM\n✅ Logs de acceso habilitados para auditoría\n✅ La regla por defecto devuelve 404 para evitar exponer detalles internos\n✅ Puerto 80 redirige limpio a HTTPS (443)\n✅ Lista para integrar con WAF o CloudWatch en el futuro\n✅ Todo es Infrastructure-as-Code, listo para CI/CD\n❌ Lo Que No Usamos (Y Por Qué) Lambda@Edge – ideal para lógica compleja a escala, pero excesivo para simples 301. CloudFront Functions – menor soporte en Terraform, más limitaciones. S3 Static Hosting – sin SSL incorporado a menos que se combine con CloudFront. Al final, ganó la simplicidad. ALB cubrió el 100% de nuestro caso de uso — sin Lambda, sin CloudFront.\n📌 Consejo de Escalado AWS ALB soporta hasta 100 listener rules por listener.\nSi necesitás más:\n• 🧩 Usar múltiples listeners (por ejemplo, diferentes puertos o múltiples ALBs) • 🚀 Migrá a CloudFront Functions o Lambda@Edge para redirects a gran escala • 🗂️ Agrupá redirecciones usando dominios wildcard o patrones de ruta\nPlanificá tu estrategia de redirects desde temprano para evitar cuellos de botella en escalabilidad.\n💡 Lecciones Aprendidas Mantené la infraestructura tan simple como el problema lo requiere — NGINX era excesivo para redirecciones. Las reglas de ALB son poderosas, pero no escalan infinitamente. Planificá los límites desde el inicio. Terraform hizo que experimentar y hacer rollback fuera seguro y auditable. Depender de servicios gestionados como ACM para SSL eliminó el 90% de la carga operativa. En resumen, transformamos un setup frágil en EC2 en un sistema serverless robusto — ahorrando dinero, esfuerzo y dolores de cabeza futuros.\n🙋‍♂️ ¿Necesitás Ayuda Con Esto? ¿Estás considerando una migración similar o querés simplificar tu setup de redirecciones?\nHemos migrado este mismo patrón en múltiples cuentas de AWS — y el equipo de operaciones nunca miró atrás.\n💬 Hablemos en LinkedIn — encantado de ayudarte o intercambiar ideas. ","date":"July 14, 2025","hero":"/images/default-hero.jpg","permalink":"https://nserbin.github.io/es/posts/aws/aws-networking/from-nginx-to-alb/","summary":"\u003cp\u003eAntes de entrar en el por qué y el cómo, esto es lo que logramos:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e✅ Reemplazamos un punto único de falla con componentes nativos de AWS\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e💸 Reducimos los costos mensuales en un 70%\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🧑‍💻 No más SSH, servidores ni reinicios de NGINX\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🔐 SSL totalmente gestionado con ACM\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e🛠️ Todo codificado y versionado con Terraform\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eTodo esto — sin un solo servidor EC2 ni archivo de configuración NGINX.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"por-qué-dejamos-ec2--nginx-y-cuánto-nos-ahorramos\"\u003ePor qué dejamos EC2 + NGINX (y cuánto nos ahorramos)\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nTeníamos un objetivo simple: redirigir tráfico de más de 70 dominios diferentes.Primero lo resolvimos con lo que conocíamos — una instancia EC2 corriendo NGINX con reglas 301 y 302 hardcodeadas. Funcionó… hasta que dejó de funcionar.\u003c/p\u003e","tags":["Load Balancer","Redirects","NGINX","EC2"],"title":"NGINX Out, ALB In - 70% Cheaper Redirects with Zero Servers"}]