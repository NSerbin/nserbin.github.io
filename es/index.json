[{"categories":["AWS","Databases","Backup"],"contents":"Antes de entrar en el por quÃ© y el cÃ³mo, esto es lo que entregamos:\nâ±ï¸ DiagnÃ³stico claro en menos de un minuto (sin adivinar en la consola)\nğŸ«¥ Camino no intrusivo: solo instancia temporal (no tocamos tu RDS actual)\nğŸ” Export que funciona, con la forma correcta de IAM/KMS\nğŸ§© Comandos copy-paste para tu runbook o CI\nTodo esto sin modificar tu instancia actual: restaurÃ¡s el Snapshot en una instancia gp3 temporal ğŸš€, creÃ¡s un Snapshot nuevo ğŸ“¸ y exportÃ¡s ese ğŸ“¦. Por quÃ© el Export estÃ¡ deshabilitado ğŸ”’ El bloqueo mÃ¡s frecuente es simple: el Snapshot fue tomado desde una instancia con almacenamiento magnÃ©tico (standard) ğŸ§². Los Snapshots con ese linaje no son elegibles para Export. La consola es ambigua; el CLI te muestra exactamente quÃ© pasa ğŸ§ª.\nConsole vs CLI: lo que realmente ayuda âš–ï¸ âŒ Solo consola âœ… Runbook con CLI Feedback BotÃ³n gris, sin motivo claro ğŸ˜¶â€ğŸŒ«ï¸ Estado y campos explÃ­citos (StorageType, errores) ğŸ§¾ Acciones Prueba y error ğŸ¯ Flujo determinÃ­stico (restore â†’ Snapshot â†’ export) â–¶ï¸ Repeatability Pasos manuales ğŸ§± Scriptable \u0026 auditable ğŸ¤– DiagnÃ³stico rÃ¡pido (CLI) ğŸ•µï¸â€â™‚ï¸ Revisar el tipo de almacenamiento del Snapshot\naws rds describe-db-snapshots --db-snapshot-identifier \u0026lt;SNAPSHOT_ID\u0026gt; --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,Engine,EngineVersion,StorageType,Encrypted,Status]\u0026#39; --output table Si StorageType = standard, ese Snapshot no se va a exportar. Arquitectura a simple vista ğŸ§­ El flujo es directo: restore del Snapshot â†’ instancia gp3 temporal â†’ nuevo Snapshot â†’ Export a S3 con un IAM Role asumido por export.rds.amazonaws.com.\nEsto muestra:\nğŸ” DÃ³nde cambia el linaje de almacenamiento (magnÃ©tico â†’ gp3)\nğŸ”— CÃ³mo el servicio de export asume un IAM Role para escribir en un S3 Bucket\nğŸ§° Las piezas mÃ­nimas (RDS, S3, IAM, KMS opcional)\nPlaybook (sin cambios sobre tu RDS existente) ğŸ“˜ No modificamos tu DB actual. Restauramos el Snapshot en una instancia SSD temporal, creamos un Snapshot nuevo y exportamos ese. ğŸ§± Restore del Snapshot en gp3 (instancia temporal) # Vars export REGION=\u0026lt;REGION\u0026gt; export TEMP_DB_ID=\u0026lt;TEMP_DB_ID\u0026gt; export SNAPSHOT_ID=\u0026lt;SNAPSHOT_ID\u0026gt; export SUBNET_GROUP=\u0026lt;DB_SUBNET_GROUP\u0026gt; export SG_ID=\u0026lt;SECURITY_GROUP_ID\u0026gt; aws rds restore-db-instance-from-db-snapshot --region $REGION --db-instance-identifier $TEMP_DB_ID --db-snapshot-identifier $SNAPSHOT_ID --storage-type gp3 --no-publicly-accessible --db-subnet-group-name $SUBNET_GROUP --vpc-security-group-ids $SG_ID ğŸ“¸ Crear un nuevo Snapshot (ahora SSD) export NEW_SNAPSHOT_ID=${TEMP_DB_ID}-exportable-$(date +%Y%m%d) aws rds create-db-snapshot --db-snapshot-identifier $NEW_SNAPSHOT_ID --db-instance-identifier $TEMP_DB_ID # Sanity check aws rds describe-db-snapshots --db-snapshot-identifier $NEW_SNAPSHOT_ID --query \u0026#39;DBSnapshots[0].[DBSnapshotIdentifier,StorageType,Status]\u0026#39; --output table ğŸ§· Export a S3 (el wiring que importa) âœ… Requerimientos ğŸŒ S3 Bucket en la misma regiÃ³n que el Snapshot\nğŸ¤ IAM Role para Export con trust en export.rds.amazonaws.com\nğŸ“ El Role puede escribir en el S3 Bucket (PutObject, ListBucket, GetBucketLocation, etc.)\nğŸ”‘ Si usÃ¡s KMS, la KMS Key estÃ¡ Enabled y permite al servicio de export\nğŸ›‘ Sin SCP de la Organization bloqueando rds:StartExportTask\nğŸ‘‡ Trust Policy (asumida por el servicio de export) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;export.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } ğŸ“œ Minimal Permissions Policy (S3 + KMS) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject*\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:DeleteObject*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;KMS_KEY_ARN\u0026gt;\u0026#34; } ] } ğŸ§¾ Bucket Policy (allow explÃ­cito para el Role) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowRDSExportWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt;\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;BUCKET\u0026gt;/*\u0026#34; ] } ] } â–¶ï¸ Iniciar el Export aws rds start-export-task \\ --export-task-identifier export-$NEW_SNAPSHOT_ID \\ --source-arn arn:aws:rds:$REGION:\u0026lt;ACCOUNT_ID\u0026gt;:snapshot:$NEW_SNAPSHOT_ID \\ --s3-bucket-name \u0026lt;BUCKET\u0026gt; \\ --s3-prefix rds-exports/$NEW_SNAPSHOT_ID/ \\ --iam-role-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/\u0026lt;RDS_EXPORT_ROLE\u0026gt; \\ --kms-key-id \u0026lt;KMS_KEY_ARN\u0026gt; ğŸ‘€ Observar la ejecuciÃ³n aws rds describe-export-tasks \\ --filters Name=export-task-identifier,Values=export-$NEW_SNAPSHOT_ID \\ --query \u0026#39;ExportTasks[0].[Status,ProgressPercentage,S3Bucket,S3Prefix,FailureCause]\u0026#39; \\ --output table Troubleshooting (triage rÃ¡pido) ğŸ§¯ ğŸ” AccessDenied â†’ revisÃ¡ SCP de la Organization, permisos del caller y la Policy del IAM Role\nğŸ§± KMSKeyNotAccessible â†’ KMS Key deshabilitada o algÃºn Deny afectando export.rds.amazonaws.com\nğŸ—ºï¸ BucketRegionError â†’ Snapshot y S3 Bucket deben estar en la misma regiÃ³n\nğŸ¤ Shared snapshot â†’ alinear accesos/grants de KMS para cross-account\nğŸ’¡ Lecciones aprendidas No dependas de la consola para explicar el â€œpor quÃ©â€ â€” el CLI es tu fuente de verdad Prevenilo de entrada: estandarizÃ¡ gp3 en tus mÃ³dulos y bloqueÃ¡ standard en CI MantenÃ© un export Role preaprobado por cuenta/regiÃ³n para evitar IAM ad-hoc ğŸ™‹â€â™‚ï¸ Â¿NecesitÃ¡s ayuda con esto? Â¿NecesitÃ¡s ayuda manteniendo tu RDS â€” Backups restaurables, governance de Snapshots, exports predecibles, etc. ?\nImplementamos estos patrones en mÃºltiples cuentas y regiones de AWS â€” repetibles, auditables y de bajo mantenimiento.\nğŸ’¬ Hablemos en LinkedIn â€” encantado de ayudar o intercambiar ideas. ","date":"August 27, 2025","hero":"/images/default-hero.jpg","permalink":"https://nserbin.github.io/es/posts/aws/aws-rds/export-to-s3/","summary":"\u003cp\u003eAntes de entrar en el por quÃ© y el cÃ³mo, esto es lo que entregamos:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eâ±ï¸ DiagnÃ³stico claro en menos de un minuto (sin adivinar en la consola)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ«¥ Camino no intrusivo: solo instancia \u003cstrong\u003etemporal\u003c/strong\u003e (no tocamos tu RDS actual)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ” Export que funciona, con la forma correcta de \u003cstrong\u003eIAM/KMS\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ§© Comandos copy-paste para tu runbook o CI\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"margin-top: 2rem;\"\u003e\u003c/div\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eTodo esto sin modificar tu instancia actual: restaurÃ¡s el Snapshot en una instancia \u003cstrong\u003egp3\u003c/strong\u003e temporal ğŸš€, creÃ¡s un Snapshot nuevo ğŸ“¸ y exportÃ¡s ese ğŸ“¦.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 4rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"por-quÃ©-el-export-estÃ¡-deshabilitado-\"\u003ePor quÃ© el Export estÃ¡ deshabilitado ğŸ”’\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nEl bloqueo mÃ¡s frecuente es simple: el Snapshot fue tomado desde una instancia con almacenamiento \u003cstrong\u003emagnÃ©tico (\u003ccode\u003estandard\u003c/code\u003e)\u003c/strong\u003e ğŸ§². Los Snapshots con ese linaje \u003cstrong\u003eno\u003c/strong\u003e son elegibles para Export. La consola es ambigua; el \u003cstrong\u003eCLI\u003c/strong\u003e te muestra exactamente quÃ© pasa ğŸ§ª.\u003c/p\u003e","tags":["S3","RDS","Storage Type","Export to S3","Snapshot"],"title":"RDS Snapshot â†’ S3: Diagnosticar y resolver un Export deshabilitado"},{"categories":["AWS"],"contents":"Antes de entrar en el por quÃ© y el cÃ³mo, esto es lo que logramos:\nâœ… Reemplazamos un punto Ãºnico de falla con componentes nativos de AWS\nğŸ’¸ Reducimos los costos mensuales en un 70%\nğŸ§‘â€ğŸ’» No mÃ¡s SSH, servidores ni reinicios de NGINX\nğŸ” SSL totalmente gestionado con ACM\nğŸ› ï¸ Todo codificado y versionado con Terraform\nTodo esto â€” sin un solo servidor EC2 ni archivo de configuraciÃ³n NGINX. Por quÃ© dejamos EC2 + NGINX (y cuÃ¡nto nos ahorramos) TenÃ­amos un objetivo simple: redirigir trÃ¡fico de mÃ¡s de 70 dominios diferentes.Primero lo resolvimos con lo que conocÃ­amos â€” una instancia EC2 corriendo NGINX con reglas 301 y 302 hardcodeadas. FuncionÃ³â€¦ hasta que dejÃ³ de funcionar.\nLo que empezÃ³ como una soluciÃ³n rÃ¡pida se convirtiÃ³ en un dolor de cabeza operativo:\nCualquier nueva redirecciÃ³n requerÃ­a acceso por SSH y ediciÃ³n manual de archivos Reiniciar NGINX implicaba riesgos de downtime Un solo typo podÃ­a romper todo Y lo mÃ¡s importante: estÃ¡bamos manteniendo un servidor completo solo para manejar redirecciones Finalmente nos preguntamos: Â¿esto no es solo una planilla glorificada con SSL y problemas de uptime?Esa pregunta nos llevÃ³ a rediseÃ±ar todo con ALB, Terraform, ACM y nada mÃ¡s.\nY funcionÃ³ â€” espectacularmente. ğŸ’° EC2 vs ALB: El costo de la simplicidad âŒ EC2 + NGINX âœ… ALB + Terraform Costo ~$73/mes ~$22.50/mes Acceso Requiere SSH/SSM No requiere SSH/SSM SSL ConfiguraciÃ³n manual SSL automÃ¡tico (ACM) Mantenimiento EdiciÃ³n manual de archivos Totalmente versionado en Terraform Escalabilidad Punto Ãºnico de falla Reglas totalmente escalables Downtime Reinicios de NGINX causan downtime Cero downtime en cambios de reglas Eso representa una reducciÃ³n de ~70%, sin servidores que mantener y con escalabilidad total.\nğŸ›  La nueva soluciÃ³n: ALB + Terraform + SSL + Logs Una vez que migramos, querÃ­amos que la soluciÃ³n fuera automatizada, segura y fÃ¡cil de mantener. Esto fue lo que construimos:\nğŸ“Š Vista general: cÃ³mo fluye el trÃ¡fico A veces es mÃ¡s fÃ¡cil entender este tipo de infraestructura visualmente. AcÃ¡ tenÃ©s un diagrama de alto nivel que muestra cÃ³mo viajan los requests del usuario a travÃ©s de Route 53, el ALB, y cÃ³mo se manejan las redirecciones:\nEste diagrama muestra:\nQuÃ© pasa cuando alguien accede a un dominio conocido o no registrado CÃ³mo Route 53 enruta los requests al ALB La diferencia entre el trÃ¡fico que llega por el puerto 80 y el puerto 443 QuÃ© hace el ALB cuando no encuentra una regla de redirecciÃ³n (retorna 404) ğŸ”§ Componentes utilizados ALB (Application Load Balancer): Maneja todos los requests HTTP/HTTPS Redirect Listener Rules: Definen quÃ© se redirige y hacia dÃ³nde Route 53: Hosted Zones ya manejaban el DNS de todos nuestros dominios AWS ACM: Para emitir y renovar automÃ¡ticamente los certificados SSL Access Logs: Activados y enviados a S3 para auditorÃ­a Terraform: Usado para definir toda la infraestructura como cÃ³digo Modules: Usamos los mÃ³dulos de Anton Babenko para alb y acm ğŸ” GestiÃ³n de certificados SSL con ACM Utilizamos ACM (AWS Certificate Manager) para solicitar certificados por dominio, con renovaciÃ³n automÃ¡tica activada. Como ya tenÃ­amos los dominios en Route 53, la validaciÃ³n se hizo automÃ¡ticamente por DNS â€” sin configuraciÃ³n manual.\nCÃ³digo:\nmodule \u0026#34;acm\u0026#34; { source = \u0026#34;terraform-aws-modules/acm/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; domain_name = \u0026#34;example.com\u0026#34; subject_alternative_names = [\u0026#34;www.example.com\u0026#34;] zone_id = data.aws_route53_zone.main.zone_id validate_certificate = true } ğŸŒ Listener Rules + Redirects con ALB Implementamos dos listeners:\nPuerto 80 (HTTP): Redirige todo a HTTPS\nPuerto 443 (HTTPS): Procesa reglas de redirect (mÃ¡ximo 100), con acciÃ³n por defecto 404\nCÃ³digo:\nmodule \u0026#34;redirect_alb\u0026#34; { source = \u0026#34;terraform-aws-modules/alb/aws\u0026#34; version = \u0026#34;9.12.0\u0026#34; name = \u0026#34;redirect-alb\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; internal = false vpc_id = aws_vpc.main.id subnets = [aws_subnet.public_a.id, aws_subnet.public_b.id] access_logs = { bucket = aws_s3_bucket.alb_logs.bucket enabled = true prefix = \u0026#34;alb-access\u0026#34; } create_security_group = true security_group_ingress_rules = { http = { from_port = 80 to_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } https = { from_port = 443 to_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } http_listeners = [ { port = 80 protocol = \u0026#34;HTTP\u0026#34; action = { type = \u0026#34;redirect\u0026#34; redirect = { port = \u0026#34;443\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; } } } ] https_listeners = [ { port = 443 protocol = \u0026#34;HTTPS\u0026#34; certificate_arn = module.acm.certificate_arn rules = [ { priority = 10 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.example.com\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;old-domain.com\u0026#34;] } }] }, { priority = 20 actions = [{ type = \u0026#34;redirect\u0026#34; redirect = { host = \u0026#34;www.blog.example.com\u0026#34; path = \u0026#34;/blog\u0026#34; status_code = \u0026#34;HTTP_301\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; } }] conditions = [{ host_header = { values = [\u0026#34;legacy-blog.net\u0026#34;] } }] } ] default_action = { type = \u0026#34;fixed-response\u0026#34; fixed_response = { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Not Found\u0026#34; status_code = \u0026#34;404\u0026#34; } } } ] } âœ¨ Por quÃ© funcionÃ³ para nosotros âœ… Completamente serverless â€” sin EC2 que mantener\nâœ… ~70% de ahorro en costos\nâœ… Todos los redirects estÃ¡n versionados con Terraform\nâœ… Certificados SSL gestionados automÃ¡ticamente (con renovaciones) vÃ­a ACM\nâœ… Logs de acceso habilitados para auditorÃ­a\nâœ… La regla por defecto devuelve 404 para evitar exponer detalles internos\nâœ… Puerto 80 redirige limpio a HTTPS (443)\nâœ… Lista para integrar con WAF o CloudWatch en el futuro\nâœ… Todo es Infrastructure-as-Code, listo para CI/CD\nâŒ Lo Que No Usamos (Y Por QuÃ©) Lambda@Edge â€“ ideal para lÃ³gica compleja a escala, pero excesivo para simples 301. CloudFront Functions â€“ menor soporte en Terraform, mÃ¡s limitaciones. S3 Static Hosting â€“ sin SSL incorporado a menos que se combine con CloudFront. Al final, ganÃ³ la simplicidad. ALB cubriÃ³ el 100% de nuestro caso de uso â€” sin Lambda, sin CloudFront.\nğŸ“Œ Consejo de Escalado AWS ALB soporta hasta 100 listener rules por listener.\nSi necesitÃ¡s mÃ¡s:\nâ€¢ ğŸ§© Usar mÃºltiples listeners (por ejemplo, diferentes puertos o mÃºltiples ALBs) â€¢ ğŸš€ MigrÃ¡ a CloudFront Functions o Lambda@Edge para redirects a gran escala â€¢ ğŸ—‚ï¸ AgrupÃ¡ redirecciones usando dominios wildcard o patrones de ruta\nPlanificÃ¡ tu estrategia de redirects desde temprano para evitar cuellos de botella en escalabilidad.\nğŸ’¡ Lecciones Aprendidas MantenÃ© la infraestructura tan simple como el problema lo requiere â€” NGINX era excesivo para redirecciones. Las reglas de ALB son poderosas, pero no escalan infinitamente. PlanificÃ¡ los lÃ­mites desde el inicio. Terraform hizo que experimentar y hacer rollback fuera seguro y auditable. Depender de servicios gestionados como ACM para SSL eliminÃ³ el 90% de la carga operativa. En resumen, transformamos un setup frÃ¡gil en EC2 en un sistema serverless robusto â€” ahorrando dinero, esfuerzo y dolores de cabeza futuros.\nğŸ™‹â€â™‚ï¸ Â¿NecesitÃ¡s Ayuda Con Esto? Â¿EstÃ¡s considerando una migraciÃ³n similar o querÃ©s simplificar tu setup de redirecciones?\nHemos migrado este mismo patrÃ³n en mÃºltiples cuentas de AWS â€” y el equipo de operaciones nunca mirÃ³ atrÃ¡s.\nğŸ’¬ Hablemos en LinkedIn â€” encantado de ayudarte o intercambiar ideas. ","date":"July 14, 2025","hero":"/images/default-hero.jpg","permalink":"https://nserbin.github.io/es/posts/aws/aws-networking/from-nginx-to-alb/","summary":"\u003cp\u003eAntes de entrar en el por quÃ© y el cÃ³mo, esto es lo que logramos:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eâœ… Reemplazamos un punto Ãºnico de falla con componentes nativos de AWS\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ’¸ Reducimos los costos mensuales en un 70%\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ§‘â€ğŸ’» No mÃ¡s SSH, servidores ni reinicios de NGINX\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ” SSL totalmente gestionado con ACM\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eğŸ› ï¸ Todo codificado y versionado con Terraform\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eTodo esto â€” sin un solo servidor EC2 ni archivo de configuraciÃ³n NGINX.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"por-quÃ©-dejamos-ec2--nginx-y-cuÃ¡nto-nos-ahorramos\"\u003ePor quÃ© dejamos EC2 + NGINX (y cuÃ¡nto nos ahorramos)\u003c/h2\u003e\n\u003cp\u003e\u003cdiv style=\"margin-top: 1rem;\"\u003e\u003c/div\u003e\nTenÃ­amos un objetivo simple: redirigir trÃ¡fico de mÃ¡s de 70 dominios diferentes.Primero lo resolvimos con lo que conocÃ­amos â€” una instancia EC2 corriendo NGINX con reglas 301 y 302 hardcodeadas. FuncionÃ³â€¦ hasta que dejÃ³ de funcionar.\u003c/p\u003e","tags":["Load Balancer","Redirects","NGINX","EC2"],"title":"NGINX Out, ALB In - 70% Cheaper Redirects with Zero Servers"}]